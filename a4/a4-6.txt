import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
white_wine_data = pd.read_csv(url, sep=';')

# Select the input and output variables
X = white_wine_data[['volatile acidity', 'alcohol', 'density']]
y = white_wine_data['quality']

# Split the data into training and testing sets
# Here we'll use the entire dataset for simplicity
X_train, y_train = X, y

# Train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_train)

# Calculate the intercept, coefficients, RMSE, and R^2 score
intercept = model.intercept_
coefficients = model.coef_
rmse = mean_squared_error(y_train, y_pred, squared=False)
r2 = r2_score(y_train, y_pred)

print(f'Intercept: {intercept}')
print(f'Coefficients: {coefficients}')
print(f'RMSE: {rmse}')
print(f'R^2: {r2}')

# Plot the data points and the regression line for each feature
features = ['volatile acidity', 'alcohol', 'density']
plt.figure(figsize=(18, 5))

for i, feature in enumerate(features):
    plt.subplot(1, 3, i + 1)
    plt.scatter(X[feature], y, color='blue', label='Data points')
    
    # Create a linear space for the feature to plot the regression line
    x_space = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)
    # Fix: Create a DataFrame for prediction that matches the training data structure
    x_full = pd.DataFrame({
        f: x_space.flatten() if f == feature else np.full(100, X[f].mean()) 
        for f in features
    })
    y_pred_line = model.predict(x_full)
    
    plt.plot(x_space, y_pred_line, color='red', linewidth=2, label='Regression line')
    plt.xlabel(feature)
    plt.ylabel('Quality')
    plt.title(f'Regression Line ({feature} vs Quality)')
    plt.legend()

plt.tight_layout()
plt.show()
